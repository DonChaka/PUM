{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC\n",
    "from typing import Callable, Tuple, List, Optional\n",
    "from IPython.core.display_functions import display\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, RocCurveDisplay, auc, roc_auc_score, f1_score, balanced_accuracy_score\n",
    "from time import time_ns, time\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "random_state = 244827\n",
    "n_samples = 2427\n",
    "n_trains = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class PUMData:\n",
    "    def __init__(self, gen, random_state=244827, test_size=0.2):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(gen[0], gen[1], test_size=test_size, random_state=random_state, shuffle=True, stratify=gen[1])\n",
    "\n",
    "    def normalize(self, min_v=0, max_v=1):\n",
    "        for i in range(self.x_train.shape[1]):\n",
    "            if self.x_train[:, i].dtype not in [int, float]:\n",
    "                continue\n",
    "\n",
    "            x_min = np.min(self.x_train[:, i])\n",
    "            x_max = np.max(self.x_train[:, i])\n",
    "            if x_min != x_max:\n",
    "                self.x_train[:, i] = (self.x_train[:, i] - x_min) / (x_max - x_min)\n",
    "                self.x_train[:, i] = self.x_train[:, i] * (max_v - min_v) + min_v\n",
    "\n",
    "                self.x_test[:, i] = (self.x_test[:, i] - x_min) / (x_max - x_min)\n",
    "                self.x_test[:, i] = self.x_test[:, i] * (max_v - min_v) + min_v\n",
    "            else:\n",
    "                self.x_train[:, i] = 1\n",
    "                self.x_test[:, i] = 1\n",
    "\n",
    "    def change_labels(self, old: list, new: list):\n",
    "        for o, n in zip(old, new):\n",
    "            self.y_test[self.y_test == o] = n\n",
    "            self.y_train[self.y_train == o] = n\n",
    "\n",
    "    def winsorize(self):\n",
    "        for i in range(self.x_train.shape[1]):\n",
    "            if self.x_train[:, i].dtype not in [int, float] or len(np.unique(self.x_train[:, i])) == 2:\n",
    "                continue\n",
    "            q1_train = np.percentile(self.x_train[:, i], 25)\n",
    "            q3_train = np.percentile(self.x_train[:, i], 75)\n",
    "            iqr_train = q3_train - q1_train\n",
    "            self.x_train[:, i] = np.clip(self.x_train[:, i], q1_train - 1.5 * iqr_train, q3_train + 1.5 * iqr_train)\n",
    "\n",
    "            q1_test = np.percentile(self.x_test[:, i], 25)\n",
    "            q3_test = np.percentile(self.x_test[:, i], 75)\n",
    "            iqr_test = q3_test - q1_test\n",
    "            self.x_test[:, i] = np.clip(self.x_test[:, i], q1_test - 1.5 * iqr_test, q3_test + 1.5 * iqr_test)\n",
    "\n",
    "    def make_polynomial(self, poly_degree=2, include_bias=False):\n",
    "        self.x_test = PolynomialFeatures(degree=poly_degree, include_bias=include_bias).fit_transform(self.x_test)\n",
    "        self.x_train = PolynomialFeatures(degree=poly_degree, include_bias=include_bias).fit_transform(self.x_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class ActivationFunction(ABC):\n",
    "    threshold = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x: ndarray) -> ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, x: ndarray) -> ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def d(self, x: ndarray) -> ndarray:\n",
    "        return self.derivative(x)\n",
    "\n",
    "class Heaviside(ActivationFunction):\n",
    "    def __call__(self, x: ndarray) -> ndarray:\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "    def derivative(self, x: ndarray) -> ndarray:\n",
    "        return np.ones_like(x)\n",
    "\n",
    "class Sin(ActivationFunction):\n",
    "    def __call__(self, x: ndarray) -> ndarray:\n",
    "        return np.sin(x)\n",
    "\n",
    "    def derivative(self, x: ndarray) -> ndarray:\n",
    "        return np.cos(x)\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def __call__(self, x: ndarray) -> ndarray:\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def derivative(self, x: ndarray) -> ndarray:\n",
    "        return 1 / (np.cosh(x) ** 2)\n",
    "\n",
    "class Sign(ActivationFunction):\n",
    "    def __call__(self, x: ndarray) -> ndarray:\n",
    "        return np.sign(x)\n",
    "\n",
    "    def derivative(self, x: ndarray) -> ndarray:\n",
    "        return np.ones_like(x)\n",
    "\n",
    "class Relu(ActivationFunction):\n",
    "    def __call__(self, x: ndarray) -> ndarray:\n",
    "        return np.where(x >= 0, x, 0)\n",
    "\n",
    "    def derivative(self, x: ndarray) -> ndarray:\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "class LeakyRelu(ActivationFunction):\n",
    "    threshold = 0.01\n",
    "\n",
    "    def __call__(self, x: ndarray) -> ndarray:\n",
    "        return np.where(x >= 0, x, 0.01 * x)\n",
    "\n",
    "    def derivative(self, x: ndarray) -> ndarray:\n",
    "        return np.where(x >= 0, 1, 0.01)\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    threshold = 0.5\n",
    "\n",
    "    def __call__(self, x: ndarray) -> ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x: ndarray) -> ndarray:\n",
    "        return self(x) * (1 - self(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,2) and (64,1) not aligned: 2 (dim 1) != 64 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 140>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    137\u001B[0m data_moons \u001B[38;5;241m=\u001B[39m PUMData(make_moons(n_samples\u001B[38;5;241m=\u001B[39mn_samples, noise\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m, random_state\u001B[38;5;241m=\u001B[39mrandom_state))\n\u001B[0;32m    139\u001B[0m model \u001B[38;5;241m=\u001B[39m NeuralNetwork(output_shapes\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m1\u001B[39m], activations\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m--> 140\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43msingle_mod_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msingle_mod_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36mNeuralNetwork.fit\u001B[1;34m(self, X, Y)\u001B[0m\n\u001B[0;32m     98\u001B[0m Y_batch \u001B[38;5;241m=\u001B[39m batched_Y[i:i \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size]\n\u001B[0;32m    100\u001B[0m preds, pre_activations, activations \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward(X_batch)\n\u001B[1;32m--> 101\u001B[0m deltas \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_deltas\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpre_activations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_activations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mY_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredicted\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    102\u001B[0m dW \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_back_propagate(deltas\u001B[38;5;241m=\u001B[39mdeltas, activations\u001B[38;5;241m=\u001B[39mactivations)\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28mprint\u001B[39m(dW\u001B[38;5;241m.\u001B[39mshape)\n",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36mNeuralNetwork._deltas\u001B[1;34m(self, pre_activations, Y, predicted)\u001B[0m\n\u001B[0;32m     67\u001B[0m deltas[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m d_loss\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(deltas)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 69\u001B[0m     delta \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[43ml\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimators\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdeltas\u001B[49m\u001B[43m[\u001B[49m\u001B[43ml\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[l]\u001B[38;5;241m.\u001B[39mactivation\u001B[38;5;241m.\u001B[39md(pre_activations[l])\n\u001B[0;32m     70\u001B[0m     deltas[l] \u001B[38;5;241m=\u001B[39m delta\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m deltas\n",
      "\u001B[1;31mValueError\u001B[0m: shapes (3,2) and (64,1) not aligned: 2 (dim 1) != 64 (dim 0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\", line 1035, in _pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\n",
      "  File \"D:\\JetBrains\\apps\\PyCharm-P\\ch-0\\213.7172.26\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py\", line 144, in cmd_step_over\n",
      "    if _is_inside_jupyter_cell(frame, pydb):\n",
      "  File \"D:\\JetBrains\\apps\\PyCharm-P\\ch-0\\213.7172.26\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py\", line 209, in _is_inside_jupyter_cell\n",
      "    if is_cell_filename(filename):\n",
      "  File \"D:\\JetBrains\\apps\\PyCharm-P\\ch-0\\213.7172.26\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py\", line 220, in is_cell_filename\n",
      "    ipython_shell = get_ipython()\n",
      "NameError: name 'get_ipython' is not defined\n"
     ]
    }
   ],
   "source": [
    "class DenseLayer:\n",
    "    _accepted_activation_functions = ['Heaviside', 'sin', 'tanh', 'sign', 'Relu', 'LeakyRelu', 'sigmoid']\n",
    "    def __init__(self, input_features: int, n_neurons: int, activation: str):\n",
    "        self.n_neurons = n_neurons\n",
    "        self.input_features = input_features\n",
    "        self.activation: ActivationFunction = {'Heaviside': Heaviside(),\n",
    "                           'sin': Sin(),\n",
    "                           'tanh': Tanh(),\n",
    "                           'sign': Sign(),\n",
    "                           'Relu': Relu(),\n",
    "                           'LeakyRelu': LeakyRelu(),\n",
    "                           'sigmoid': Sigmoid()}[activation]\n",
    "\n",
    "        self.estimators: ndarray = np.random.normal(loc=1, scale=.15, size=(n_neurons, input_features+1))\n",
    "\n",
    "    def forward(self, X: ndarray) -> ndarray:\n",
    "        X = self.expand_features(X)\n",
    "        return X.dot(self.estimators.T)\n",
    "\n",
    "    def activate(self, S: ndarray) -> ndarray:\n",
    "        return self.activation(S)\n",
    "\n",
    "    def predict(self, X: ndarray) -> ndarray:\n",
    "        X = self.expand_features(X)\n",
    "        return self.activate(X.dot(self.estimators.T))\n",
    "\n",
    "    @staticmethod\n",
    "    def expand_features(X: ndarray) -> ndarray:\n",
    "        return np.c_[np.ones(X.shape[0]) * -1, X]\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: List[DenseLayer], learning_rate: float = 0.01, min_lr: float=0.001, max_lr=0.01, batch_size=64, max_epochs=1000, verbose=False, dynamic_lr=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_learning_rate = min_lr\n",
    "        self.max_learning_rate = max_lr\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.dynamic_lr = dynamic_lr\n",
    "        self.epochs = max_epochs\n",
    "        self.layers = layers\n",
    "\n",
    "    def _forward(self, X: ndarray) -> Tuple[ndarray, ndarray, ndarray]:\n",
    "        pre_activations = []\n",
    "        activated = []\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "            pre_activations.append(X)\n",
    "            X = layer.activate(X)\n",
    "            activated.append(X)\n",
    "        return X, pre_activations, activated\n",
    "\n",
    "    def _deltas(self, pre_activations: ndarray, Y: ndarray, predicted: ndarray):\n",
    "        d_loss = self._derivative_loss_function(Y, predicted) * self.layers[-1].activation.d(pre_activations[-1])\n",
    "        deltas = [0] * (len(self.layers) - 1)\n",
    "        deltas[-1] = d_loss\n",
    "        for l in range(len(deltas)-2, -1, -1):\n",
    "            delta = self.layers[l + 1].estimators.T.dot(deltas[l + 1]) * self.layers[l].activation.d(pre_activations[l])\n",
    "            deltas[l] = delta\n",
    "        return deltas\n",
    "\n",
    "    def _back_propagate(self, deltas , activations):\n",
    "        dw = []\n",
    "        deltas = [0] + deltas\n",
    "        for l in range(1, len(self.layers)):\n",
    "            dw_l = deltas[l].dot(activations[l -1].T)\n",
    "            dw.append(dw_l)\n",
    "\n",
    "        return dw\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        Y = Y.reshape(-1, 1)\n",
    "        n = X.shape[0]\n",
    "        batch_divisible = n - n % self.batch_size\n",
    "\n",
    "        for e in tqdm(range(self.epochs), disable=not self.verbose):\n",
    "            indexes = np.arange(0, n, 1)\n",
    "            np.random.shuffle(indexes)\n",
    "            batched_X = X[indexes]\n",
    "            batched_Y = Y[indexes]\n",
    "\n",
    "            batched_X = batched_X[:batch_divisible]\n",
    "            batched_Y = batched_Y[:batch_divisible]\n",
    "\n",
    "            for i in range(0, batch_divisible, self.batch_size):\n",
    "                X_batch = batched_X[i:i + self.batch_size]\n",
    "                Y_batch = batched_Y[i:i + self.batch_size]\n",
    "\n",
    "                preds, pre_activations, activations = self._forward(X_batch)\n",
    "                deltas = self._deltas(pre_activations=pre_activations, Y=Y_batch, predicted=preds)\n",
    "                dW = self._back_propagate(deltas=deltas, activations=activations)\n",
    "\n",
    "                print(dW.shape)\n",
    "\n",
    "                # dW = (Y_batch - preds) * self.activation.d(X_batch.dot(self.coef_.T)) * X_batch\n",
    "                # self.coef_ += self.learning_rate * dW.mean(axis=0)\n",
    "\n",
    "            if self.dynamic_lr:\n",
    "                self.learning_rate = self.min_learning_rate + (self.max_learning_rate - self.min_learning_rate) * (\n",
    "                            1 + np.cos(e / self.epochs * np.pi))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X: ndarray) -> ndarray:\n",
    "        output = X\n",
    "        for estimator in self.layers:\n",
    "            output = estimator.predict(X)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def predict(self, X: ndarray) -> ndarray:\n",
    "        return np.where(self.decision_function(X) > self.layers[-1].activation.threshold, 1, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _loss_function(true: ndarray, predicted: ndarray) -> ndarray:\n",
    "        n = predicted.shape[1]\n",
    "        cost = (1. / (2 * n)) * np.sum((true - predicted) ** 2)\n",
    "        return cost\n",
    "\n",
    "    @staticmethod\n",
    "    def _derivative_loss_function(true: ndarray, predicted: ndarray) -> ndarray:\n",
    "        return predicted - true\n",
    "\n",
    "\n",
    "single_mod_data = PUMData(make_classification(n_samples=n_samples, n_features=2, n_redundant=0, n_classes=2, n_clusters_per_class=1, random_state=random_state), random_state=random_state)\n",
    "data_moons = PUMData(make_moons(n_samples=n_samples, noise=0.05, random_state=random_state))\n",
    "\n",
    "model = NeuralNetwork([\n",
    "    DenseLayer(2, 4, 'sigmoid'),\n",
    "    DenseLayer(4, 8, 'sigmoid'),\n",
    "    DenseLayer(8, 1, 'sigmoid')\n",
    "])\n",
    "model.fit(single_mod_data.x_train, single_mod_data.y_train)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%class Nclass NeuralNetwork:\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}